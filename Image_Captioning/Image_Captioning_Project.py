# -*- coding: utf-8 -*-
"""image captioning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GaQW1nOBZNgKqSgxDw_gq4zRfugBndBp
"""

!pip install opendatasets
!pip install keras
!pip install tensorflow

import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Concatenate, TimeDistributed,MultiHeadAttention
from tensorflow.keras.layers import RepeatVector,Lambda


from google.colab import drive
drive.mount('/content/drive')

import opendatasets as od
od.download("https://www.kaggle.com/datasets/eeshawn/flickr30k?resource=download")


!ls /content/drive/MyDrive/

"""**Load Cations**"""

import pandas as pd
import numpy as np
data=pd.read_csv('/content/flickr30k/captions.txt')
df=pd.DataFrame(data)
df=df.drop("comment_number",axis=1)
df["comment"] = df["comment"].str.replace(".", "", regex=False)










df['comment'] = 'startseq ' + df['comment'].str.lower().str.strip() + ' endseq'

df['comment'][:5]

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=12000,oov_token="Nothing")

df['comment'] = df['comment'].str.lower().str.strip()
tokenizer.fit_on_texts(df['comment'])


# import itertools
# rare_words = list(itertools.islice(tokenizer.word_index.items(), 11500, 18000))
# print(rare_words[:10])



vocab_size=min(12000,len(tokenizer.word_index)+1)

sequences = tokenizer.texts_to_sequences(df["comment"])

max_seq_len = max(len(seq) for seq in sequences)
max_seq_len

from tensorflow.keras.preprocessing.sequence import pad_sequences

max_seq_len = max(len(seq) for seq in sequences)
max_seq_len
paded_seq=pad_sequences(sequences,maxlen=max_seq_len,padding='post',truncating='post')
# paded_seq[:1]

"""imports"""

import os
import numpy as np
import tensorflow as tf
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.models import Model
from tqdm import tqdm

"""train test split"""

# Get unique image IDs
unique_ids = df['image_name'].unique()

# Split into train and test
train_ids, test_ids = train_test_split(unique_ids, test_size=0.2, random_state=42)

print("Train size:", len(train_ids))
print("Test size:", len(test_ids))

print(train_ids[:5])

"""feature extraction"""

# Load ResNet50 without final classification layer
base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')
cnn_encoder = Model(inputs=base_model.input, outputs=base_model.output)
# Input size must be (224, 224, 3) for ResNet50

def preprocess_image(img_path):
    img = load_img(img_path, target_size=(224, 224))
    img = img_to_array(img)
    img = np.expand_dims(img, axis=0)        # Add batch dimension
    img = preprocess_input(img)              # ResNet50-specific
    return img

def extract_features(image_ids, image_dir):
    features = {}
    batch_size = 32
    batch_images = []
    batch_names = []

    for img_id in tqdm(image_ids):
        img_path = os.path.join(image_dir, img_id)
        if not os.path.exists(img_path):
            print("Missing:", img_path)  # optional debug
            continue

        img = preprocess_image(img_path)
        batch_images.append(img)
        batch_names.append(img_id)

        # When batch is full or last image
        if len(batch_images) == batch_size or img_id == image_ids[-1]:
            batch_tensor = np.vstack(batch_images)
            batch_features = cnn_encoder.predict(batch_tensor)

            for name, feat in zip(batch_names, batch_features):
                features[name] = feat

            # Clear batch
            batch_images = []
            batch_names = []

    return features


image_dir = "/content/flickr30k/flickr30k_images"  # <-- Your folder

train_features = extract_features(train_ids, image_dir)
test_features  = extract_features(test_ids, image_dir)

print("Train image features:", len(train_features))
print("Test image features:", len(test_features))

print(len(train_features))  # Total number of images
print(len(next(iter(train_features.values()))))  # Length of feature vector, should be 2048

random_id = list(train_features.keys())[0]
print(f"Sample feature vector for '{random_id}':\n{train_features[random_id][:10]}")  # print first 10 values

"""save features"""

import pickle

with open("train_image_features.pkl", "wb") as f:
    pickle.dump(train_features, f)

with open("test_image_features.pkl", "wb") as f:
    pickle.dump(test_features, f)

train_df = df[df['image_name'].isin(train_ids)].copy()
test_df = df[df['image_name'].isin(test_ids)].copy()
train_df.head()

# import pickle

# # Load training features
# with open("/content/drive/MyDrive/train_image_features.pkl", "rb") as f:
#     train_features = pickle.load(f)

# # Load test features
# with open("/content/drive/MyDrive/test_image_features.pkl", "rb") as f:
#     test_features = pickle.load(f)
# # train_df.head(20)
# # train_features

# with open("/content/drive/MyDrive/train_image_features.pkl", "rb") as f:
#     data = f.read()
#     print("File size (bytes):", len(data))

# from tensorflow.keras.preprocessing.sequence import pad_sequences



from tensorflow.keras.preprocessing.sequence import pad_sequences
sample=[]
def full_sequence_data_generator(df, tokenizer, image_features, max_seq_len, batch_size=32):
    vocab_size = len(tokenizer.word_index) + 1  # vocab size for output shape

    def generator():
        while True:
            df_shuffled = df.sample(frac=1).reset_index(drop=True)
            X1, X2, y, sw = [], [], [], []

            for i in range(len(df_shuffled)):
                row = df_shuffled.iloc[i]
                caption = row['comment']
                img_id = row['image_name']

                if img_id not in image_features:
                    continue

                tokens = tokenizer.texts_to_sequences([caption])[0]
                tokens = tokens[:max_seq_len - 1]  # reserve 1 slot for <end>
                tokens.append(0)  # assume <end> token is 0 (optional)

                input_seq = pad_sequences([tokens[:-1]], maxlen=max_seq_len, padding='post')[0]
                output_seq = pad_sequences([tokens[1:]], maxlen=max_seq_len, padding='post')[0]

                X1.append(image_features[img_id])
                X2.append(input_seq)
                y.append(output_seq)

                # sample_weight: ignore padded positions
                mask = [1.0 if token != 0 else 0.0 for token in output_seq]
                sw.append(mask)

                if len(X1) == batch_size:
                    yield (
                        (np.array(X1, dtype=np.float32), np.array(X2, dtype=np.int32)),
                        np.array(y, dtype=np.int32),
                        np.array(sw, dtype=np.float32)
                    )
                    X1, X2, y, sw = [], [], [], []

            if len(X1) > 0:
                yield (
                    (np.array(X1, dtype=np.float32), np.array(X2, dtype=np.int32)),
                    np.array(y, dtype=np.int32),
                    np.array(sw, dtype=np.float32)
                )


    output_signature = (
        (
            tf.TensorSpec(shape=(None, 2048), dtype=tf.float32),
            tf.TensorSpec(shape=(None, max_seq_len), dtype=tf.int32)
        ),
        tf.TensorSpec(shape=(None, max_seq_len), dtype=tf.int32),              # Full output sequence
        tf.TensorSpec(shape=(None, max_seq_len), dtype=tf.float32)            # Sample weights (mask)
    )

    return tf.data.Dataset.from_generator(generator, output_signature=output_signature)


# Parameters
embedding_dim = 256
lstm_units = 256

# Image branch
image_input = Input(shape=(2048,))
img_dense = Dense(embedding_dim, activation='relu')(image_input)
img_dropout = Dropout(0.2)(img_dense)  # Reduced from 0.5

# Repeat vector to match sequence length
img_vec = RepeatVector(max_seq_len)(img_dropout)  # Shape: (batch, max_seq_len, features)
img_vec = Lambda(lambda x: tf.cast(x, tf.float32))(img_vec)

# Text branch
text_input = Input(shape=(max_seq_len,))
text_embed = Embedding(vocab_size, embedding_dim)(text_input)
text_dropout = Dropout(0.2)(text_embed)  # Reduced from 0.5

# Concatenate image features at each time step
merged = Concatenate(axis=-1)([img_vec, text_dropout])

# LSTM with sequence output
lstm1 = LSTM(512, return_sequences=True, dropout=0.3)(merged)
lstm2 = LSTM(512, return_sequences=True, dropout=0.3)(lstm1)

lstm2 = Lambda(lambda x: tf.cast(x, tf.float32))(lstm2)
img_vec = Lambda(lambda x: tf.cast(x, tf.float32))(img_vec)
# Attention layer: Use LSTM output as query, image as value
attention_layer = MultiHeadAttention(num_heads=4, key_dim=embedding_dim)
attention_output = attention_layer(query=lstm2, value=img_vec, key=img_vec)
# Combine attention output with LSTM output
context = Concatenate()([lstm2, attention_output])

# Final output layer and Time-distributed output layer(choose one)

# Replace this:
# Predict only the next word

# outputs = Dense(vocab_size, activation='softmax')(context[:, -1, :])

# With this (TimeDistributed for sequence output):
outputs = TimeDistributed(Dense(vocab_size, activation='softmax'))(context)



model = Model(inputs=[image_input, text_input], outputs=outputs)
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer='adam'
)
model.summary()

total_captions = sum([len(tokenizer.texts_to_sequences([caption])[0]) - 1 for caption in train_df['comment']])
steps_per_epoch = total_captions // 32
print(total_captions,steps_per_epoch)

# Train the model
# Create the generator
train_generator = full_sequence_data_generator(
    df=train_df,
    tokenizer=tokenizer,
    image_features=train_features,
    max_seq_len=max_seq_len,
    batch_size=32
).repeat()

from tensorflow.keras.callbacks import (
    ModelCheckpoint,
    EarlyStopping,
    ReduceLROnPlateau,
    TensorBoard,
    CSVLogger
)
import datetime
import os

# 1. Create a directory for saving logs and models
log_dir = "training_logs/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
os.makedirs(log_dir, exist_ok=True)

# 2. Define callbacks
callbacks = [
    # Save the best model (monitors validation loss if validation_data is provided)
    ModelCheckpoint(
        filepath=os.path.join(log_dir, 'best_model.h5'),
        monitor='loss',  # Change to 'val_loss' if using validation
        save_best_only=True,
        mode='min',
        verbose=1
    ),

    # Stop training if no improvement for 5 epochs
    EarlyStopping(
        monitor='loss',  # Change to 'val_loss' if using validation
        patience=5,
        restore_best_weights=True,
        verbose=1
    ),

    # Reduce learning rate when loss plateaus
    ReduceLROnPlateau(
        monitor='loss',  # Change to 'val_loss' if using validation
        factor=0.1,
        patience=3,
        min_lr=1e-6,
        verbose=1
    )


]

# 3. Add callbacks to model.fit()


# Fit the model
model.fit(
    train_generator,
    steps_per_epoch=steps_per_epoch,
    epochs=15,
    callbacks=callbacks,

    verbose=1,
)


# Save the model
model.save("caption_model.h5")

